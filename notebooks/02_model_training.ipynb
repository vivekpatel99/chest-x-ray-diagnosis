{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-02T07:06:22.039762Z",
     "iopub.status.busy": "2025-03-02T07:06:22.039402Z",
     "iopub.status.idle": "2025-03-02T07:06:32.200919Z",
     "shell.execute_reply": "2025-03-02T07:06:32.199836Z",
     "shell.execute_reply.started": "2025-03-02T07:06:22.039734Z"
    }
   },
   "source": [
    "# NIH Chest X-ray Multi label Binary classification using Tensorflow Densenet121 (Transfer learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/chest-x-ray-diagnosis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Go to project root folder\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:25:58.836148Z",
     "iopub.status.busy": "2025-03-02T10:25:58.835783Z",
     "iopub.status.idle": "2025-03-02T10:26:14.649285Z",
     "shell.execute_reply": "2025-03-02T10:26:14.647955Z",
     "shell.execute_reply.started": "2025-03-02T10:25:58.836120Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 08:03:06.424150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741075386.435442    2383 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741075386.439235    2383 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:14.652600Z",
     "iopub.status.busy": "2025-03-02T10:26:14.651810Z",
     "iopub.status.idle": "2025-03-02T10:26:14.662952Z",
     "shell.execute_reply": "2025-03-02T10:26:14.661875Z",
     "shell.execute_reply.started": "2025-03-02T10:26:14.652529Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')], '2.18.0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_gpu = tf.config.list_physical_devices('GPU')\n",
    "if not found_gpu:\n",
    "    raise Exception(\"No GPU found\")\n",
    "found_gpu, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:14.665526Z",
     "iopub.status.busy": "2025-03-02T10:26:14.665122Z",
     "iopub.status.idle": "2025-03-02T10:26:15.927292Z",
     "shell.execute_reply": "2025-03-02T10:26:15.926295Z",
     "shell.execute_reply.started": "2025-03-02T10:26:14.665487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import opendatasets as od\n",
    "\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# auto reload libs\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:15.929464Z",
     "iopub.status.busy": "2025-03-02T10:26:15.928853Z",
     "iopub.status.idle": "2025-03-02T10:26:15.934204Z",
     "shell.execute_reply": "2025-03-02T10:26:15.933185Z",
     "shell.execute_reply.started": "2025-03-02T10:26:15.929433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 320\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://www.kaggle.com/datasets/nih-chest-xrays/sample'\n",
    "\n",
    "# Look into the data directory\n",
    "datasets = 'datasets/sample'\n",
    "dataset_path = Path(datasets)\n",
    "IMAGE_DIR = dataset_path /'sample/images'\n",
    "CSV_PATH = dataset_path /'sample/sample_labels.csv'\n",
    "dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "if not dataset_path.is_dir():\n",
    "    od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/sample/sample/images\n"
     ]
    }
   ],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "# https://gist.github.com/bdsaglam/586704a98336a0cf0a65a6e7c247d248\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../conf\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "    print(cfg.DATASET_DIRS.TRAIN_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Atelectasis  Effusion  Infiltration  Mass  Nodule\n",
      "0            0         0             0     0       0\n",
      "1            1         1             1     0       0\n",
      "2            0         0             0     0       0\n",
      "3            0         0             0     0       0\n",
      "4            0         0             1     0       0\n"
     ]
    }
   ],
   "source": [
    "from utils.chest_x_ray_preprocessor import ChestXRayPreprocessor\n",
    "\n",
    "preprocessor = ChestXRayPreprocessor(cfg)\n",
    "train_ds, valid_ds, test_ds, pos_weights, neg_weights = preprocessor.get_preprocessed_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 320, 320, 3) (12, 15)\n",
      "tf.Tensor(\n",
      "[[[-1.0892599  -1.0892599  -1.0892599 ]\n",
      "  [-1.1530176  -1.1530176  -1.1530176 ]\n",
      "  [-1.3071464  -1.3071464  -1.3071464 ]\n",
      "  ...\n",
      "  [-0.23227446 -0.23227446 -0.23227446]\n",
      "  [-0.2411862  -0.2411862  -0.2411862 ]\n",
      "  [-0.25740695 -0.25740695 -0.25740695]]\n",
      "\n",
      " [[-0.9662309  -0.9662309  -0.9662309 ]\n",
      "  [-0.9005775  -0.9005775  -0.9005775 ]\n",
      "  [-0.95811623 -0.95811623 -0.95811623]\n",
      "  ...\n",
      "  [-0.23006429 -0.23006429 -0.23006429]\n",
      "  [-0.24542373 -0.24542373 -0.24542373]\n",
      "  [-0.26421484 -0.26421484 -0.26421484]]\n",
      "\n",
      " [[-0.8271123  -0.8271123  -0.8271123 ]\n",
      "  [-0.75552446 -0.75552446 -0.75552446]\n",
      "  [-0.71787786 -0.71787786 -0.71787786]\n",
      "  ...\n",
      "  [-0.2240171  -0.2240171  -0.2240171 ]\n",
      "  [-0.23195426 -0.23195426 -0.23195426]\n",
      "  [-0.25150064 -0.25150064 -0.25150064]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.72461647 -0.72461647 -0.72461647]\n",
      "  [-0.68322694 -0.68322694 -0.68322694]\n",
      "  [-0.6184415  -0.6184415  -0.6184415 ]\n",
      "  ...\n",
      "  [-0.15838882 -0.15838882 -0.15838882]\n",
      "  [-0.21605061 -0.21605061 -0.21605061]\n",
      "  [-0.24570222 -0.24570222 -0.24570222]]\n",
      "\n",
      " [[-0.6940554  -0.6940554  -0.6940554 ]\n",
      "  [-0.63319325 -0.63319325 -0.63319325]\n",
      "  [-0.5739706  -0.5739706  -0.5739706 ]\n",
      "  ...\n",
      "  [-0.18017654 -0.18017654 -0.18017654]\n",
      "  [-0.22215708 -0.22215708 -0.22215708]\n",
      "  [-0.2580455  -0.2580455  -0.2580455 ]]\n",
      "\n",
      " [[-0.6622659  -0.6622659  -0.6622659 ]\n",
      "  [-0.59574395 -0.59574395 -0.59574395]\n",
      "  [-0.5372257  -0.5372257  -0.5372257 ]\n",
      "  ...\n",
      "  [-0.1812604  -0.1812604  -0.1812604 ]\n",
      "  [-0.2072223  -0.2072223  -0.2072223 ]\n",
      "  [-0.24675353 -0.24675353 -0.24675353]]], shape=(320, 320, 3), dtype=float32) tf.Tensor([0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1.], shape=(15,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    images, labels = batch\n",
    "    print(images.shape, labels.shape)\n",
    "    print(images[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:15.935705Z",
     "iopub.status.busy": "2025-03-02T10:26:15.935348Z",
     "iopub.status.idle": "2025-03-02T10:26:16.017730Z",
     "shell.execute_reply": "2025-03-02T10:26:16.016598Z",
     "shell.execute_reply.started": "2025-03-02T10:26:15.935679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5606 entries, 0 to 5605\n",
      "Data columns (total 11 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Image Index                  5606 non-null   object \n",
      " 1   Finding Labels               5606 non-null   object \n",
      " 2   Follow-up #                  5606 non-null   int64  \n",
      " 3   Patient ID                   5606 non-null   int64  \n",
      " 4   Patient Age                  5606 non-null   object \n",
      " 5   Patient Gender               5606 non-null   object \n",
      " 6   View Position                5606 non-null   object \n",
      " 7   OriginalImageWidth           5606 non-null   int64  \n",
      " 8   OriginalImageHeight          5606 non-null   int64  \n",
      " 9   OriginalImagePixelSpacing_x  5606 non-null   float64\n",
      " 10  OriginalImagePixelSpacing_y  5606 non-null   float64\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 481.9+ KB\n"
     ]
    }
   ],
   "source": [
    "sample_df = pd.read_csv(CSV_PATH)\n",
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.019091Z",
     "iopub.status.busy": "2025-03-02T10:26:16.018822Z",
     "iopub.status.idle": "2025-03-02T10:26:16.025872Z",
     "shell.execute_reply": "2025-03-02T10:26:16.024849Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.019070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_csv= 'datasets/valid-small.csv'\n",
    "test_csv= 'datasets/test.csv'\n",
    "\n",
    "valid_df = pd.read_csv(f\"{valid_csv}\")\n",
    "test_df = pd.read_csv(f\"{test_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.drop('PatientId', axis=1, inplace=True)\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.072006Z",
     "iopub.status.busy": "2025-03-02T10:26:16.071673Z",
     "iopub.status.idle": "2025-03-02T10:26:16.081586Z",
     "shell.execute_reply": "2025-03-02T10:26:16.080481Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.071976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_df['Patient ID'].count(),sample_df['Patient ID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Droping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.084119Z",
     "iopub.status.busy": "2025-03-02T10:26:16.083741Z",
     "iopub.status.idle": "2025-03-02T10:26:16.100944Z",
     "shell.execute_reply": "2025-03-02T10:26:16.099944Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.084090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_df.drop_duplicates(subset=['Patient ID'], inplace=True)\n",
    "sample_df['Patient ID'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_labels = ['Image Index', 'Finding Labels']\n",
    "sample_df = sample_df[sample_labels]\n",
    "sample_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cat_df = sample_df['Finding Labels'].str.get_dummies(sep='|').astype('float32')\n",
    "sample_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.027432Z",
     "iopub.status.busy": "2025-03-02T10:26:16.027004Z",
     "iopub.status.idle": "2025-03-02T10:26:16.068794Z",
     "shell.execute_reply": "2025-03-02T10:26:16.067670Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.027391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare new csv file with useful information and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.115278Z",
     "iopub.status.busy": "2025-03-02T10:26:16.114971Z",
     "iopub.status.idle": "2025-03-02T10:26:16.137019Z",
     "shell.execute_reply": "2025-03-02T10:26:16.135960Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.115253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "useful_data_df= sample_df[['Image Index', 'Finding Labels']]\n",
    "useful_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.138586Z",
     "iopub.status.busy": "2025-03-02T10:26:16.138160Z",
     "iopub.status.idle": "2025-03-02T10:26:16.156456Z",
     "shell.execute_reply": "2025-03-02T10:26:16.155469Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.138521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(useful_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.157721Z",
     "iopub.status.busy": "2025-03-02T10:26:16.157404Z",
     "iopub.status.idle": "2025-03-02T10:26:16.172399Z",
     "shell.execute_reply": "2025-03-02T10:26:16.171369Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.157686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_most_common_label(labels):\n",
    "    return Counter(labels.split('|')).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.211619Z",
     "iopub.status.busy": "2025-03-02T10:26:16.211254Z",
     "iopub.status.idle": "2025-03-02T10:26:16.230012Z",
     "shell.execute_reply": "2025-03-02T10:26:16.228974Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.211584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "useful_data_df = useful_data_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.231428Z",
     "iopub.status.busy": "2025-03-02T10:26:16.231054Z",
     "iopub.status.idle": "2025-03-02T10:26:16.247053Z",
     "shell.execute_reply": "2025-03-02T10:26:16.245926Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.231397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images_df = useful_data_df['Image Index'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.270249Z",
     "iopub.status.busy": "2025-03-02T10:26:16.269889Z",
     "iopub.status.idle": "2025-03-02T10:26:16.288399Z",
     "shell.execute_reply": "2025-03-02T10:26:16.287342Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.270221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.289767Z",
     "iopub.status.busy": "2025-03-02T10:26:16.289406Z",
     "iopub.status.idle": "2025-03-02T10:26:16.312092Z",
     "shell.execute_reply": "2025-03-02T10:26:16.311104Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.289739Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Perform stratified split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(images_df, useful_data_df.primary_label, test_size=0.1, random_state=42, stratify=useful_data_df.primary_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_colums = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n",
    "#        'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding',\n",
    "#        'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']\n",
    "# 'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n",
    "\n",
    "labels =['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n",
    "       'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass',\n",
    "       'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.313372Z",
     "iopub.status.busy": "2025-03-02T10:26:16.313015Z",
     "iopub.status.idle": "2025-03-02T10:26:16.348492Z",
     "shell.execute_reply": "2025-03-02T10:26:16.347636Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.313343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_cat_labels_df = useful_data_df['Finding Labels'].str.get_dummies(sep='|').astype('float32')\n",
    "train_cat_labels_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cat_labels_df.drop(['No Finding'], inplace=True, axis=1)\n",
    "# labels = train_cat_labels_df.columns\n",
    "train_cat_labels_df = train_cat_labels_df[labels]\n",
    "train_cat_labels_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_images = valid_df.Image\n",
    "valid_labels_df = valid_df[labels]\n",
    "valid_labels_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Dataset from Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.421428Z",
     "iopub.status.busy": "2025-03-02T10:26:16.421129Z",
     "iopub.status.idle": "2025-03-02T10:26:16.440520Z",
     "shell.execute_reply": "2025-03-02T10:26:16.439246Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.421404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_image(image_name, label):\n",
    "    full_path = tf.strings.join([f'{IMAGE_DIR}/', image_name])\n",
    "    image = tf.io.read_file(full_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])  # Resize to the desired size\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "def load_image_valid(image_name, label):\n",
    "    full_path = tf.strings.join(['datasets/images-small/', image_name])\n",
    "    image = tf.io.read_file(full_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])  # Resize to the desired size\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.442118Z",
     "iopub.status.busy": "2025-03-02T10:26:16.441753Z",
     "iopub.status.idle": "2025-03-02T10:26:16.463368Z",
     "shell.execute_reply": "2025-03-02T10:26:16.462383Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.442086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(images_df), len(train_cat_labels_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.485002Z",
     "iopub.status.busy": "2025-03-02T10:26:16.484630Z",
     "iopub.status.idle": "2025-03-02T10:26:16.617935Z",
     "shell.execute_reply": "2025-03-02T10:26:16.616940Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.484966Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((images_df,  train_cat_labels_df.values))\n",
    "train_ds = train_ds.map(load_image, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.619484Z",
     "iopub.status.busy": "2025-03-02T10:26:16.619088Z",
     "iopub.status.idle": "2025-03-02T10:26:16.646012Z",
     "shell.execute_reply": "2025-03-02T10:26:16.645067Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.619445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_images.values,  valid_labels_df.values))\n",
    "valid_ds = valid_ds.map(load_image_valid, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:16.647307Z",
     "iopub.status.busy": "2025-03-02T10:26:16.647032Z",
     "iopub.status.idle": "2025-03-02T10:26:28.384636Z",
     "shell.execute_reply": "2025-03-02T10:26:28.383307Z",
     "shell.execute_reply.started": "2025-03-02T10:26:16.647286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create normalization layer\n",
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "# Compute the mean and variance using the training data\n",
    "# We need to convert the dataset to numpy to compute statistics\n",
    "images_for_stats = []\n",
    "for images, _ in train_ds.take(int(len(images_df)*0.25)): \n",
    "    images_for_stats.append(images)\n",
    "images_for_stats = tf.concat(images_for_stats, axis=0)\n",
    "normalization_layer.adapt(images_for_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:28.385899Z",
     "iopub.status.busy": "2025-03-02T10:26:28.385612Z",
     "iopub.status.idle": "2025-03-02T10:26:28.406274Z",
     "shell.execute_reply": "2025-03-02T10:26:28.405255Z",
     "shell.execute_reply.started": "2025-03-02T10:26:28.385874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomRotation(0.10),                    # Small rotation\n",
    "    tf.keras.layers.RandomTranslation(0.05, 0.05),           # Translation\n",
    "    tf.keras.layers.RandomContrast(0.1),                     # Contrast adjustment\n",
    "    tf.keras.layers.RandomBrightness(0.1),                   # Brightness adjustment\n",
    "    tf.keras.layers.RandomZoom(0.1, 0.1)])                   # Random zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:28.407743Z",
     "iopub.status.busy": "2025-03-02T10:26:28.407355Z",
     "iopub.status.idle": "2025-03-02T10:26:28.412685Z",
     "shell.execute_reply": "2025-03-02T10:26:28.411448Z",
     "shell.execute_reply.started": "2025-03-02T10:26:28.407705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, label):\n",
    "    image = tf.cast(image, tf.float32) #/ 255.0\n",
    "    # image = tf.keras.applications.densenet.preprocess_input(image)\n",
    "    image = normalization_layer(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:28.414089Z",
     "iopub.status.busy": "2025-03-02T10:26:28.413782Z",
     "iopub.status.idle": "2025-03-02T10:26:28.767983Z",
     "shell.execute_reply": "2025-03-02T10:26:28.766796Z",
     "shell.execute_reply.started": "2025-03-02T10:26:28.414063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda image, label: (data_augmentation(image), label) , num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.map(preprocess_image,num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE).shuffle(len(images_df))\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T10:26:28.769369Z",
     "iopub.status.busy": "2025-03-02T10:26:28.769002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    print(batch[0].shape)\n",
    "    image, label = batch\n",
    "    print(image[0], label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "valid_ds = valid_ds.map(preprocess_image,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
    "valid_ds = valid_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x=labels, height=np.mean(train_cat_labels_df.values, axis=0))\n",
    "plt.title(\"Frequency of Each Class\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Class Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = train_cat_labels_df.shape[0]\n",
    "positive_frequencies = (train_cat_labels_df==1).sum()/N\n",
    "negative_frequencies = (train_cat_labels_df==0).sum()/N\n",
    "positive_frequencies, negative_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(list(positive_frequencies.items()), columns=['class', 'positives'])\n",
    "data_df['negatives'] = negative_frequencies.values\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.plot.bar(x='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. \n",
    "\n",
    "To have this, we want \n",
    "\n",
    "$$w_{pos} \\times freq_{p} = w_{neg} \\times freq_{n},$$\n",
    "\n",
    "which we can do simply by taking \n",
    "\n",
    "$$w_{pos} = freq_{neg}$$\n",
    "$$w_{neg} = freq_{pos}$$\n",
    "\n",
    "This way, we will be balancing the contribution of positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = negative_frequencies.values\n",
    "neg_weights = positive_frequencies.values\n",
    "# positive_frequencies.values\n",
    "\n",
    "# Try adjusting the weight balance slightly\n",
    "# pos_weights = np.sqrt(negative_frequencies.values) * 0.8  # Reduce positive weight slightly\n",
    "# neg_weights = np.sqrt(positive_frequencies.values) * 1.2  # Increase negative weight slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_contirbution = positive_frequencies * pos_weights\n",
    "neg_contribution = negative_frequencies * neg_weights\n",
    "\n",
    "pos_contirbution, neg_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_data_df = pd.DataFrame(list(pos_contirbution.items()), columns=['class', 'positives'])\n",
    "weighted_data_df['negatives'] = neg_contribution.values\n",
    "weighted_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_data_df.plot.bar(x='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted loss calculation to handle class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. Now let's implement such a loss function. \n",
    "\n",
    "After computing the weights, our final weighted loss for each training case will be \n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Return weighted loss function given negative weights and positive weights.\n",
    "\n",
    "    Args:\n",
    "      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n",
    "      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n",
    "    \n",
    "    Returns:\n",
    "      weighted_loss (function): weighted loss function\n",
    "    \"\"\"\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Return weighted loss value. \n",
    "\n",
    "        Args:\n",
    "            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n",
    "            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n",
    "        Returns:\n",
    "            loss (float): overall scalar loss summed across all classes\n",
    "        \"\"\"\n",
    "        # initialize loss to zero\n",
    "        loss = 0.0\n",
    "\n",
    "        for i in range(len(pos_weights)):\n",
    "            y = y_true[:, i]\n",
    "            f_of_x = y_pred[:, i]\n",
    "\n",
    "            f_of_x_log = K.log(f_of_x + epsilon)\n",
    "            f_of_x_1_min_log = K.log((1-f_of_x) + epsilon)\n",
    "\n",
    "            first_term = pos_weights[i] * y * f_of_x_log\n",
    "            sec_term = neg_weights[i] * (1-y) * f_of_x_1_min_log\n",
    "            loss_per_col = - K.mean(first_term + sec_term)\n",
    "            loss += loss_per_col\n",
    "        return loss\n",
    "\n",
    "    return weighted_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare DenseNet121 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'imagenet',\n",
    "\n",
    "base_model = DenseNet121(\n",
    "     include_top=False,\n",
    "     weights='pretrain_weights/densenet.hdf5', #'imagenet', \n",
    "     input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)  \n",
    ")\n",
    "# base_model.trainable = False\n",
    "\n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "# and a logistic layer\n",
    "predictions = Dense(len(labels), activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='AUC'), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 50\n",
    "# model = build_model()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), \n",
    "               loss=get_weighted_loss(pos_weights, neg_weights),\n",
    "        metrics=METRICS)     \n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECK_POINT_DIR = 'exported_models'\n",
    "checkpoint_prefix = os.path.join(CHECK_POINT_DIR, \"ckpt_{epoch}\")\n",
    "LOG_DIR = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True,\n",
    "                                        save_best_only=True,\n",
    "                                        monitor='val_loss',\n",
    "                                        mode='min'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, \n",
    "                    validation_data=valid_ds,\n",
    "                    epochs = EPOCHS,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune the Model\n",
    "Right now, our model is sort of smart. We'll change the learning rate so it doesn't jump to conclusions too quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.trainable = True\n",
    "# fine_tune_at = 149 \n",
    "\n",
    "# # Freeze all layers before the fine_tune_at layer\n",
    "# for layer in base_model.layers[:fine_tune_at]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Recompile the model with a lower learning rate\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "#     loss=get_weighted_loss(pos_weights, neg_weights),\n",
    "#     metrics=METRICS\n",
    "# )\n",
    "\n",
    "# # Set the number of epochs for fine-tuning\n",
    "# fine_tune_epochs = 20\n",
    "# total_epochs = EPOCHS + fine_tune_epochs  # Total epochs\n",
    "\n",
    "# # Continue training the model with fine-tuning\n",
    "# history_fine = model.fit(\n",
    "#     train_ds,\n",
    "#     epochs=total_epochs,\n",
    "#     initial_epoch=history.epoch[-1],  # Start from the last epoch of initial training\n",
    "#     validation_data=valid_ds,\n",
    "#     callbacks=callbacks  \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train_loss', 'val_loss'])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Training Accuracy Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4667,
     "sourceId": 7773,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
